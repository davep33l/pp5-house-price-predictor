{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Evaluate the missing data\n",
    "* Clean the data\n",
    "  \n",
    "## Inputs\n",
    "\n",
    "* Dataset: `outputs/datasets/collection/house_prices_records.csv`\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Generate a fully cleaned set for study, and cleaned Train and Test set.\n",
    "  * `outputs/datasets/cleaned/CompleteSetCleaned.csv`\n",
    "  * `outputs/datasets/cleaned/TrainSetCleaned.csv`\n",
    "  * `outputs/datasets/cleaned/TestSetCleaned.csv`\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "* Perform the following Imputations\n",
    "  * Drop Features - `EnclosedPorch`, `WoodDeckSF`\n",
    "  * Median Imputation - `LotFrontage`, `GarageYrBlt`\n",
    "  * Mean Imputation - `BedroomAbvGr`\n",
    "  * Arbitrary Imputation (Zero) - `2ndFlrSF`, `MasVnrArea`\n",
    "  * Categorical Imputation (Mode) - `GarageFinish`, `BsmtFinType1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all modules required in the workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ppscore as pps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropFeatures\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebooks for this project are stored in a subfolder called `jupyter_notebooks`, therefore when running the notebook, the working directory needs to be changed to the parent folder. \n",
    "* We access the current directory with `os.getcwd()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory of: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_path = \"outputs/datasets/collection/house_prices_records.csv\"\n",
    "df = pd.read_csv(df_raw_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the distribution and shape of the variables with missing data. Also added a comparison of variables without missing data to review differences in data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_without_missing_data = df.columns[df.isna().sum() == 0].to_list()\n",
    "vars_without_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[vars_with_missing_data].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[vars_with_missing_data].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[vars_without_missing_data].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Noticed that the data types with missing data are either float or object, and the data types without missing data are int or object. This is potentially due to the nature in which NaN is recorded as a float64 type. We can review these inconsistencies at the end of the data cleaning process, as you can assume all values of square feet are recorded as whole integers and not as decimal numbers. This can address the performance and consistency point referred to at the end of `1_Data_Collection.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vars_with_missing_data:\n",
    "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "    profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"There are no variables with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Review**\n",
    "* 2ndFlrSF missing 5.9% of values, numeric, 53.5% zero's, min=0 : max=2065\n",
    "* BedroomAbvGr missing 6.8% of values, numeric, 0.4% zero's, min=0 : max=8 (why are there zeros for bedrooms?)\n",
    "* BsmtFinType1 missing 7.8% of values, categorical (7 categories), none=31 instances : unf=396 instances\n",
    "* EnclosedPorch missing 90.7% of values, numeric, 7.9% zero's, min=0 : max=286, (consider dropping?)\n",
    "* GarageFinish missing 11.1% of values, categorical (4 categories), none=73 (5.6%) instances : unf=546 (42.1%) instances\n",
    "* GarageYrBlt missing 5.5% of values, numeric, 0.0% zero's, min=1900 : max=2010\n",
    "* LotFrontage missing 17.7% of values, numeric, 0.0% zero's, min=21 : max=313\n",
    "* MasVnrArea missing 0.5% of values, numeric, 59.0% zero's, min=0 : max=1600\n",
    "* WoodDeckSF missing 89.4% of values, numeric, 5.3% zero's, min=0 : max=736 (consider dropping?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We run a correlation and PPS Analysis to help guide the data cleaning steps further in the workflow. Allowing us to determine the impact missing data affects the relationships between variables, and to help direct which variables need closer attention when performing imputation in the data cleaning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                    linewidth=0.5\n",
    "                    )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                         linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    df_corr_spearman = df.corr(method=\"spearman\")\n",
    "    df_corr_pearson = df.corr(method=\"pearson\")\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20, 12), font_annot=8):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          \"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Correlations and Power Predictive Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As this review is conducted on unclean data, it was expected that there would be some warnings which have indicated there are potentially some imbalance in the data. This will be picked up during the comprehesive cleaning process and re-assessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCorrAndPPS(df_corr_pearson=df_corr_pearson,\n",
    "                  df_corr_spearman=df_corr_spearman,\n",
    "                  pps_matrix=pps_matrix,\n",
    "                  CorrThreshold=0.4,\n",
    "                  PPS_Threshold=0.2,\n",
    "                  figsize=(12, 10),\n",
    "                  font_annot=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Findings**\n",
    "\n",
    "* Spearman correlation:\n",
    "  * Very Strong Correlation (0.7 - 1.0)\n",
    "    * Features vs SalePrice\n",
    "      * highlights that there is very strong correlation between `OverallQual` and `GrLivArea` vs `SalePrice`\n",
    "    * Multi-colinearity\n",
    "      * highlights that there is very strong multi-colinearity between `YearBuilt` and `GarageYrBlt`\n",
    "  * highlights that there is very strong multi-colinearity between `TotalBsmtSF` and `1stFlrSF`\n",
    "  * Strong Correlation (0.5 - 0.7) - *summarised for anything > 0.6\n",
    "    * Features vs SalePrice\n",
    "      * highlights that there is strong correlation between `1stFlrSF`, `GarageArea`, `GarageYrBlt`, `TotalBsmtSF`, `YearBuilt` and `YearRemodAdd`\n",
    "    * Multi-colinearity\n",
    "      * highlights that there is strong multi-colinearity between `GrLivArea` and `2ndFlrSF`\n",
    "      * highlights that there is strong multi-colinearity between `OverallQual` and `GarageYrBlt`\n",
    "      * highlights that there is strong multi-colinearity between `OverallQual` and `GrLivArea`\n",
    "      * highlights that there is strong multi-colinearity between `OverallQual` and `YearBuilt`\n",
    "      * highlights that there is strong multi-colinearity between `YearBuilt` and `YearRemodAdd`\n",
    "* Pearson correlation:\n",
    "  * Very Strong Correlation (0.7 - 1.0)\n",
    "    * Features vs SalePrice\n",
    "      * highlights that there is very strong correlation between `OverallQual` and `GrLivArea` vs `SalePrice`\n",
    "    * Multi-colinearity\n",
    "      * highlights that there is very strong multi-colinearity between `YearBuilt` and `GarageYrBlt`\n",
    "      * highlights that there is very strong multi-colinearity between `TotalBsmtSF` and `1stFlrSF`\n",
    "  * Strong Correlation (0.5 - 0.7) - *summarised for anything > 0.6\n",
    "    * Features vs SalePrice\n",
    "      * highlights that there is strong correlation between `1stFlrSF`, `GarageArea`, `GarageYrBlt`, `TotalBsmtSF`, `YearBuilt` and `YearRemodAdd`\n",
    "    * Multi-colinearity\n",
    "      * highlights that there is strong multi-colinearity between `GrLivArea` and `2ndFlrSF`\n",
    "* Power Predictive Score\n",
    "  * `GarageYrBlt` indicates it is a strong predictor of `YearBuilt` with a PPS of 0.70\n",
    "  * `YearBuilt` indicates it is a moderate predictor of `GarageYrBlt` with with a PPS of 0.63\n",
    "  * `OverallQual` indicates it is a moderate predictor of `KitchenQual` of with a PPS of 0.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Missing Data Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Custom function to display missing data levels in a DataFrame, it shows the absolute levels, relative levels and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMissingData(df):\n",
    "    missing_data_absolute = df.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
    "    df_missing_data = (pd.DataFrame(data={\n",
    "                                        \"RowsWithMissingData\": missing_data_absolute,\n",
    "                                        \"PercentageOfDataset\": missing_data_percentage,\n",
    "                                        \"DataType\": df.dtypes}\n",
    "                                    ).sort_values(by=['PercentageOfDataset'], ascending=False)\n",
    "                                     .query(\"PercentageOfDataset > 0\")\n",
    "                       )\n",
    "\n",
    "    return df_missing_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing data levels for the collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two variables in consideration for dropping, due to high levels of missing data. Those being `EnclosedPorch` & `WoodDeckSF`. They are unlikely to give any reliable information by trying to impute the missing values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load in a function to help with asssessing the Data Cleaning process. The function was provided by code institute as part of the Predictive Analysis Course. \n",
    "* The overall objective of the function is to assess the effect of the cleaning process for different types of imputations. Mean, Median and Arbitrary Imputations on Numerical values, and when replacing Categorical variables with \"Missing\" or a most frequent category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataCleaningEffect(df_original, df_cleaned, variables_applied_with_method):\n",
    "\n",
    "    # Indicate plot number\n",
    "    flag_count = 1\n",
    "\n",
    "    # distinguish between numerical and categorical variables\n",
    "    categorical_variables = df_original.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "    # scan over variables,\n",
    "    # first on variables that you applied the method\n",
    "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
    "    for set_of_variables in [variables_applied_with_method]:\n",
    "        print(\"\\n=====================================================================================\")\n",
    "        print(\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
    "        print(f\"{set_of_variables} \\n\\n\")\n",
    "\n",
    "        for var in set_of_variables:\n",
    "            if var in categorical_variables:\n",
    "\n",
    "                df1 = pd.DataFrame({\"Type\": \"Original\", \"Value\": df_original[var]})\n",
    "                df2 = pd.DataFrame({\"Type\": \"Cleaned\", \"Value\": df_cleaned[var]})\n",
    "                dfAux = pd.concat([df1, df2], axis=0)\n",
    "                fig, axes = plt.subplots(figsize=(15, 5))\n",
    "                sns.countplot(hue='Type', data=dfAux, x=\"Value\", palette=['#432371', \"#FAAE7B\"])\n",
    "                axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "                plt.xticks(rotation=90)\n",
    "                plt.legend()\n",
    "\n",
    "            else:\n",
    "\n",
    "                fig, axes = plt.subplots(figsize=(10, 5))\n",
    "                sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True, element=\"step\", ax=axes)\n",
    "                sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True, element=\"step\", ax=axes)\n",
    "                axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
    "                plt.legend()\n",
    "\n",
    "            plt.show()\n",
    "            flag_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EnclosedPorch - Drop Missing Data\n",
    "\n",
    "* For the EnclosedPorch, I will drop this variable due to the high quantity of missing data (89.38%), and with it being unlikely to impute any reliable information from it. \n",
    "\n",
    "#### WoodDeckSF - Drop Missing Data\n",
    "\n",
    "* For the EnclosedPorch, I will drop this variable due to the high quantity of missing data (89.38%), and with it being unlikely to impute any reliable information from it.\n",
    "\n",
    "#### LotFrontage - Median Imputation\n",
    "\n",
    "* For the LotFrontage, I will Impute using Median as even though the data has close Mean (69) and Median (70.04995837) values, it has a high positive Skewness (2.163569142) indicating a long right tail and also has high positive Kurtosis (17.45286726) which implies there is a presence of outliers. Therefore Median would be the best approach as its less affected by extreme values compared to the mean. \n",
    "\n",
    "#### BedroomAbvGr - Mean Imputation\n",
    "\n",
    "* For the BedroomAbvGr, I will Impute using Mean due to the Mean and Median being close indicates a normal distribution. This is supported by the Skewness (0.23) being close to zero, indicating the feature is not skewed and close to normal. Mean imputation is also supported by Kurtosis (2.32), which implies the tails are shorter and not many outliers. \n",
    "\n",
    "#### 2ndFlrSF - Arbitrary Number Imputation (using Zero)\n",
    "\n",
    "* For the 2ndFlrSF, as there are no other features to assist in the imputation of this, as ideally a feature that described the number of floors would be helpful. I have decided to use an arbitrary imputation of zero due to the already existing large proportion of the values being zero. Implying houses do not have a second floor. There is correlation for both Spearman (0.65) and Pearson (0.69) between the GrLivArea and 2ndFlrSF, but this cannot be used effectively to impute due to so many zero values in the data.  The only other approach that would seem feasible is to obtain additional information on the houses for the amount of floors, but as the data set does not contain this information, I am considering that approach out of scope for now. \n",
    "\n",
    "#### GarageYrBlt\t- Median Imputation\n",
    "\n",
    "* For the GarageYrBlt, I will impute using Median due to Mean (1978.5) and Median (1980) being close indicates a fairly normal distribution, but as the data shows slight skewness (-0.65) and light tails from the kurtosis calculation (-0.42), median imputation seems best. I could also explore more advanced imputation if the model does not perform by using the correlated variable YearBuilt. \n",
    "\n",
    "#### MasVnrArea\t- Arbitrary Number Imputation (using Zero)\n",
    "\n",
    "* For the MasVnrArea, I have decided to use an arbitrary imputation of zero due to the already existing large proportion of the values being zero (59%). Also due to the skewness (2.67) indicating most of the values are close to zero and kurtosis (10.08) indicating lots of outliers. \n",
    "\n",
    "#### GarageFinish - Categorical Imputation (Mode)\n",
    "\n",
    "* For the GarageFinish, as I did not perform any initial encoding during the first iteration of correlation and PPS analysis I will simply impute this feature with the most frequent value. I will revisit this method if the model is not performing and there is any correlation within the data to suggest a more indepth method might give better results (like using \"missing\" or using correlated variables). \n",
    "\n",
    "\n",
    "#### BsmtFinType1 - Categorical Imputation (Mode)\n",
    "\n",
    "* For the BsmtFinType1, as I did not perform any initial encoding during the first iteration of correlation and PPS analysis I will simply impute this feature with the most frequent value. I will revisit this method if the model is not performing and there is any correlation within the data to suggest a more indepth method might give better results (like using \"missing\" or using correlated variables). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet, TestSet, _, __ = train_test_split(\n",
    "                                        df,\n",
    "                                        df['SalePrice'],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the missing data differences between train, test and full data, by merging the data sets and calculating the percentage point difference between Train and Full sets, and Test and Full sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_missing_data = EvaluateMissingData(df)\n",
    "train_missing_data = EvaluateMissingData(TrainSet)\n",
    "test_missing_data = EvaluateMissingData(TestSet)\n",
    "\n",
    "missing_data_comparison = pd.merge(test_missing_data,\n",
    "                                   train_missing_data,\n",
    "                                   left_index=True,\n",
    "                                   right_index=True,\n",
    "                                   suffixes=('_Test', '_Train')\n",
    "                                   )\n",
    "\n",
    "missing_data_comparison = pd.merge(missing_data_comparison,\n",
    "                                   full_missing_data,\n",
    "                                   left_index=True,\n",
    "                                   right_index=True\n",
    "                                   )\n",
    "\n",
    "missing_data_comparison_pct = missing_data_comparison.drop(columns=['DataType', 'DataType_Train', 'DataType_Test',\n",
    "                                                                    'RowsWithMissingData', 'RowsWithMissingData_Train',\n",
    "                                                                    'RowsWithMissingData_Test'])\n",
    "\n",
    "# Calculate percentage point differences\n",
    "missing_data_comparison_pct['PctPointDiff_Test_Full'] = (\n",
    "    missing_data_comparison_pct['PercentageOfDataset_Test'] -\n",
    "    missing_data_comparison_pct['PercentageOfDataset'])\n",
    "\n",
    "missing_data_comparison_pct['PctPointDiff_Train_Full'] = (\n",
    "    missing_data_comparison_pct['PercentageOfDataset_Train'] -\n",
    "    missing_data_comparison_pct['PercentageOfDataset'])\n",
    "\n",
    "missing_data_comparison_pct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am happy that the split represents each variable appropriately within the train and test sets vs the full set. `WoodDeckSF` is going to be dropped so not too concered with the 3.43 difference in the test set and it is expected that the percentage point differences in the Test set would be higher due to the smaller size in the data set. All Train set are +/- 1% which is acceptable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform the following Imputations\n",
    "  * Drop Features - `EnclosedPorch`, `WoodDeckSF`\n",
    "  * Median Imputation - `LotFrontage`, `GarageYrBlt`\n",
    "  * Mean Imputation - `BedroomAbvGr`\n",
    "  * Arbitrary Imputation (Zero) - `2ndFlrSF`, `MasVnrArea`\n",
    "  * Categorical Imputation (Mode) - `GarageFinish`, `BsmtFinType1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select imputation approach\n",
    "2. Select variables to apply the imputation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['EnclosedPorch', 'WoodDeckSF']\n",
    "\n",
    "print(f\"* {len(variables_method)} variables to drop \\n\\n\"\n",
    "      f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a separate DataFrame applying this imputation approach to the selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = DropFeatures(features_to_drop=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "df_method = imputer.transform(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Confirm variables have been dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TrainSet.columns.to_list())\n",
    "print(df_method.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select imputation approach\n",
    "2. Select variables to apply the imputation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['LotFrontage', 'GarageYrBlt']\n",
    "\n",
    "print(f\"* {len(variables_method)} variables to perform Median Imputation on \\n\\n\"\n",
    "      f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a separate DataFrame applying this imputation approach to the selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = MeanMedianImputer(imputation_method='median',\n",
    "                            variables=variables_method)\n",
    "df_method = imputer.fit_transform(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Assess the effect on the variable's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select imputation approach\n",
    "2. Select variables to apply the imputation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['BedroomAbvGr']\n",
    "\n",
    "print(f\"* {len(variables_method)} variables to perform Mean Imputation on \\n\\n\"\n",
    "      f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a separate DataFrame applying this imputation approach to the selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = MeanMedianImputer(imputation_method='mean',\n",
    "                            variables=variables_method)\n",
    "df_method = imputer.fit_transform(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Assess the effect on the variable's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrary Imputation (Zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select imputation approach\n",
    "2. Select variables to apply the imputation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['2ndFlrSF', 'MasVnrArea']\n",
    "\n",
    "print(f\"* {len(variables_method)} variables perform arbitrary imputation (zero) on \\n\\n\"\n",
    "      f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a separate DataFrame applying this imputation approach to the selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = ArbitraryNumberImputer(arbitrary_number=0,\n",
    "                                 variables=variables_method)\n",
    "df_method = imputer.fit_transform(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Assess the effect on the variable's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Imputation (Mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select imputation approach\n",
    "2. Select variables to apply the imputation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_method = ['GarageFinish', 'BsmtFinType1']\n",
    "\n",
    "print(f\"* {len(variables_method)} variables to perform Categorical Imputation (Mode) on \\n\\n\"\n",
    "      f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a separate DataFrame applying this imputation approach to the selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = CategoricalImputer(imputation_method='frequent',\n",
    "                             variables=variables_method)\n",
    "df_method = imputer.fit_transform(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Assess the effect on the variable's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCleaningEffect(df_original=TrainSet,\n",
    "                   df_cleaned=df_method,\n",
    "                   variables_applied_with_method=variables_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Data Cleaning Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_pipeline = Pipeline([\n",
    "    ('median_imputer', MeanMedianImputer(imputation_method='median', variables=['LotFrontage', 'GarageYrBlt'])),\n",
    "    ('mean_imputer', MeanMedianImputer(imputation_method='mean', variables=['BedroomAbvGr'])),\n",
    "    ('zero_imputer', ArbitraryNumberImputer(arbitrary_number=0, variables=['2ndFlrSF', 'MasVnrArea'])),\n",
    "    ('mode_imputer', CategoricalImputer(imputation_method='frequent', variables=['GarageFinish', 'BsmtFinType1'])),\n",
    "    ('drop_features', DropFeatures(features_to_drop=['EnclosedPorch', 'WoodDeckSF']))\n",
    "])\n",
    "\n",
    "TrainSet, TestSet = cleaning_pipeline.fit_transform(TrainSet), cleaning_pipeline.fit_transform(TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TestSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Complete Cleaned Dataset for Correlation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_cleaned_data = cleaning_pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push files to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(name='outputs/datasets/cleaned')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_cleaned_data.to_csv(\"outputs/datasets/cleaned/CompleteSetCleaned.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
